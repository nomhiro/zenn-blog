---
title: "NLWebサーバーをローカルで動かそう！"
emoji: "🚀"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["nlweb", "microsoft", "ai", "llm"]
published: false
---

https://zenn.dev/nomhiro/articles/what-is-nlweb-2025-ms-build

# PoC①：NLWebサーバーをローカルで動かそう！🚀

**前提条件**  
- **Python 3.10以上**がローカルにインストールされていること
- Azure OpenAI に **gpt-4.1, gpt-4.1-mini, text-embedding-3-small** のモデルがデプロイされていること。
  - ![](/images/poc-nlweb-20250524/2025-05-24-17-43-37.png)


### 1. コードのダウンロード

まず、NLWebのリポジトリをクローンします。

```sh
git clone https://github.com/microsoft/NLWeb
cd NLWeb
```

### 2. Python仮想環境の作成と有効化

次に、Pythonの仮想環境を作成して、有効化します。これにより、プロジェクト専用の環境で必要なパッケージが管理できます。

```sh
python -m venv myenv
myenv\Scripts\activate
```

### 3. 依存関係のインストール

NLWebの`code`フォルダに移動し、依存ライブラリのインストールを行います。この手順で、ローカルのベクトルデータベース要求も含めたすべての必要なパッケージが自動的にインストールされます。

```sh
cd code
pip install -r requirements.txt
```

### 4. 設定ファイルの準備

#### 4-1. .envファイルの作成

`.env.template`ファイルをコピーして新しい`.env`ファイルを作成します。  
ここでは、使用する**LLMエンドポイントのAPIキー**を設定します。ローカルのQdrantデータベースの変数はすでに設定済みです。

```sh
cp .env.template .env
```

以下の環境変数を設定します。
```
AZURE_OPENAI_ENDPOINT="Azure OpenAI の API エンドポイント"
AZURE_OPENAI_API_KEY="Azure OpenAI の API キー"

QDRANT_URL="http://localhost:6333"
QDRANT_API_KEY="<OPTIONAL>"
```

※ 必要に応じて、`.env`ファイル内のAPIキーなどをお使いの環境に合わせて編集してください。

#### 4-2. 設定ファイルの更新

`code/config`フォルダ内の3つの設定ファイルを更新します。これらのファイルで、使用するプロバイダーを自分の環境に合わせる必要があります。

- **config_llm.yaml**  
  - preferred_providerを、LLMプロバイダーに合わせて更新しましょう。デフォルトは「Azure OpenAI」になっています。 
  - 利用するモデル（例：4.1、4.1-mini）もここで変更します。※NLWebとしては4.1や4.1-miniの想定で作られているようです。

:::details config_llm.yaml
```yaml
preferred_provider: azure_openai

providers:
  inception:
    api_key_env: INCEPTION_API_KEY
    api_endpoint_env: INCEPTION_ENDPOINT
    models:
      high: mercury-small
      low: mercury-small

  openai:
    api_key_env: OPENAI_API_KEY
    api_endpoint_env: OPENAI_ENDPOINT
    models:
      high: gpt-4.1
      low: gpt-4.1-mini

  anthropic:
    api_key_env: ANTHROPIC_API_KEY
    models:
      high: claude-3-5-sonnet-20241022
      low: claude-3-haiku-20240307

  gemini:
    api_key_env: GCP_PROJECT
    models:
      high: chat-bison@001
      low: chat-bison-lite@001

  azure_openai:
    api_key_env: AZURE_OPENAI_API_KEY
    api_endpoint_env: AZURE_OPENAI_ENDPOINT
    api_version_env: "2024-12-01-preview"
    models:
      high: gpt-4.1
      low: gpt-4.1-mini

  llama_azure:
    api_key_env: LLAMA_AZURE_API_KEY
    api_endpoint_env: LLAMA_AZURE_ENDPOINT
    api_version_env: "2024-12-01-preview"
    models:
      high: llama-2-70b
      low: llama-2-13b

  deepseek_azure:
    api_key_env: DEEPSEEK_AZURE_API_KEY
    api_endpoint_env: DEEPSEEK_AZURE_ENDPOINT
    api_version_env: "2024-12-01-preview"
    models:
      high: deepseek-coder-33b
      low: deepseek-coder-7b

      
  snowflake:
    api_key_env: SNOWFLAKE_PAT
    api_endpoint_env: SNOWFLAKE_ACCOUNT_URL
    api_version_env: "2024-12-01"
    models:
      high: claude-3-5-sonnet
      low: llama3.1-8b
```
:::

- **config_embedding.yaml**  
  - 最上行を、希望する埋め込み（embedding）プロバイダーに更新します。デフォルトは「Azure OpenAI」で、モデルは`text-embedding-3-small`となっています。

:::details config_embedding.yaml
```yaml
preferred_provider: azure_openai

providers:
  openai:
    api_key_env: OPENAI_API_KEY
    api_endpoint_env: OPENAI_ENDPOINT
    model: text-embedding-3-small

  gemini:
    api_key_env: GCP_PROJECT
    model: GCP_EMBEDDING_MODEL

  azure_openai:
    api_key_env: AZURE_OPENAI_API_KEY
    api_endpoint_env: AZURE_OPENAI_ENDPOINT
    api_version_env: "2024-10-21"  # Specific API version for embeddings
    model: text-embedding-3-small

  snowflake:
    api_key_env: SNOWFLAKE_PAT
    api_endpoint_env: SNOWFLAKE_ACCOUNT_URL
    api_version_env: "2024-10-01"
    model: snowflake-arctic-embed-m-v1.5
```
:::

- **config_retrieval.yaml**  
  - こちらは、ローカル環境で動かすため、「qdrant_local」に変更します。
  （デフォルトはAzure AI Searchになっています）

:::details config_retrieval.yaml
```yaml
preferred_endpoint: qdrant_local

endpoints:
  azure_ai_search:
    api_key_env: AZURE_VECTOR_SEARCH_API_KEY
    api_endpoint_env: AZURE_VECTOR_SEARCH_ENDPOINT
    index_name: embeddings1536
    db_type: azure_ai_search
    name: NLWeb_Crawl

  # Option 1: Local file-based Qdrant storage
  qdrant_local:
    # Use local file-based storage with a specific path
    database_path: "../data/db"
    # Set the collection name to use
    index_name: nlweb_collection
    # Specify the database type
    db_type: qdrant
    
  qdrant_url:
    api_endpoint_env: QDRANT_URL
    api_key_env: QDRANT_API_KEY
    index_name: nlweb_collection
    db_type: qdrant

  snowflake_cortex_search_1:
    api_key_env: SNOWFLAKE_PAT
    api_endpoint_env: SNOWFLAKE_ACCOUNT_URL
    index_name: SNOWFLAKE_CORTEX_SEARCH_SERVICE
    db_type: snowflake_cortex_search
```
:::

### 5. ベクトルデータベースへのデータロード

次に、ローカルのベクトルデータベースにサンプルデータを読み込み、テストできるようにします。以下はRSSフィードを使用した例です。  
※ 複数のRSSフィードを読み込むと、複数の「サイト」を横断して検索が可能になります。特定のサイトに検索範囲を限定する場合は`config_nlweb.yaml`で設定します。

**今回は、Azureの更新情報のサイトのRSSを使います。**
https://azure.microsoft.com/ja-jp/updates/

```sh
python -m tools.db_load https://www.microsoft.com/releasecommunications/api/v2/azure/rss azure-update
```

:::details 実行結果
```bash
(myenv) PS C:\Users\user001\Documents\poc-nlweb-20250524\NLWeb\code> python -m tools.db_load https://www.microsoft.com/releasecommunications/api/v2/azure/rss azure-update
Logger embedding_wrapper writing to: logs\embedding_wrapper.log at level ERROR
Logger azure_search_client writing to: logs\azure_search_client.log at level ERROR
Logger milvus_client writing to: logs\milvus_client.log at level ERROR
Logger qdrant_client writing to: logs\qdrant_client.log at level ERROR
Logger retriever writing to: logs\retriever.log at level ERROR
Fetching content from URL: https://www.microsoft.com/releasecommunications/api/v2/azure/rss
Fetching content from URL: https://www.microsoft.com/releasecommunications/api/v2/azure/rss 
Saved URL content to temporary file: C:\Users\user01\AppData\Local\Temp\tmpohubzupg.xml (type: rss)
Detected file type: rss, contains embeddings: No
Computing embeddings for file...
Loading data from C:\Users\user01\AppData\Local\Temp\tmpohubzupg.xml (resolved to C:\Users\user01\AppData\Local\Temp\tmpohubzupg.xml) for site azure-update using database endpoint 'qdrant_local'
Detected file type: rss
Using embedding provider: azure_openai, model: text-embedding-3-small
Processing as RSS feed...
Processing RSS/Atom feed: C:\Users\user01\AppData\Local\Temp\tmpohubzupg.xml
Processed 200 episodes from RSS/Atom feed
Computing embeddings for batch of 100 texts
Logger azure_oai_embedding writing to: logs\azure_oai_embedding.log at level ERROR
Uploading batch 1 of 2 (100 documents)
Successfully uploaded batch 1
Processed 100/200 documents
Computing embeddings for batch of 100 texts
Uploading batch 2 of 2 (100 documents)
Successfully uploaded batch 2
Processed 200/200 documents
Loading completed. Added 200 documents to the database.
Saved file with embeddings to ../data/json_with_embeddings\tmpohubzupg.xml
Cleaned up temporary file: C:\Users\user01\AppData\Local\Temp\tmpohubzupg.xml
```
:::

### 6. NLWebサーバーの起動

準備が整ったら、NLWebサーバーを起動します。  
`code`フォルダ内で以下のコマンドを実行してください。

```sh
python app-file.py
```

---

### 7. 動作確認

サーバーが正常に起動したら、ブラウザで以下のURLにアクセスしてください。
![](/images/poc-nlweb-20250524/2025-05-24-17-55-02.png)

```
http://localhost:8000/
```

シンプルな検索インターフェースが表示され、正常に動作するはずです。  
![](/images/poc-nlweb-20250524/2025-05-24-17-55-48.png)

![alt text](/images/poc-nlweb-20250524/NLWeb-Answer-Local.gif)



また、異なるサンプルUIを試す場合は、URLのパスに`static/<htmlファイル名>`を追加してアクセスしてみてください。

---

## まとめ

今回の手順では、NLWebのローカル環境でのPoC手順を紹介しました。以下のステップを通して、
1. **リポジトリのクローン＆仮想環境の構築**
2. **依存ライブラリのインストール＆環境設定**
3. **サンプルデータのデータベースへのロード**
4. **NLWebサーバーの起動と動作確認**

と進めることで、NLWebの基本機能と動作確認ができます。  
初めての方でもシンプルな手順で実行可能ですので、ぜひ試してみてください。  
これにより、対話型ウェブサイトの新しい可能性を実際に体験できます！

Happy Coding！そして、NLWebで未来のウェブ体験を楽しみましょう！

# PoC②：NLWebサーバーをAzure上で動かす