---
title: "SLM ã‹ã‚‰MCPã‚’ä½¿ã£ã¦ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™"
emoji: "ðŸ§°"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["mcp", "slm", "aifoundry"]
published: false
---

# ã¯ã˜ã‚ã«
æœ¬è¨˜äº‹ã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œã™ã‚‹å°è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ï¼ˆSLMï¼‰ã¨Model Context Protocolï¼ˆMCPï¼‰ã‚’é€£æºã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’è§£èª¬ã—ã¾ã™ã€‚

# SLMï¼ˆSmall Language Modelï¼‰ã¨ã¯
SLMã¯ã€GPT-4ã‚„Claudeãªã©ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„è¨€èªžãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ä¾‹ãˆã°Microsoft Phi-4ï¼ˆ14Bï¼‰ãªã©ãŒã‚ã‚Šã¾ã™ã€‚

- ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã§ãã‚‹ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯é…ã„ãŒã€CPUã§ã‚‚å‹•ä½œï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ã¯ãªããªã‚‹
- ãƒ‡ãƒ¼ã‚¿ãŒå¤–éƒ¨ã«é€ä¿¡ã•ã‚Œãªã„
- APIä½¿ç”¨æ–™ãŒã‹ã‹ã‚‰ãªã„

è©³ç´°ã¯ã“ã¡ã‚‰ã«ã¦ï¼
https://zenn.dev/nomhiro/articles/slm-foundry-local-poc-01#slm%EF%BC%88small-language-model%EF%BC%89%E3%81%A8%E3%81%AF%EF%BC%9F

## MCPï¼ˆModel Context Protocolï¼‰ã¨ã¯
MCPã¯Anthropicç¤¾ãŒé–‹ç™ºã—ãŸãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§ã€AIãƒ¢ãƒ‡ãƒ«ãŒå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã‚„ãƒªã‚½ãƒ¼ã‚¹ã‚’å®‰å…¨ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹æ¨™æº–ä»•æ§˜ã§ã™ã€‚

è©³ç´°ã¯ã“ã¡ã‚‰ã«ã¦ï¼
https://zenn.dev/nomhiro/articles/neo4j-mcp-person-knowledge#%EF%BC%88%E3%81%8A%E3%81%95%E3%82%89%E3%81%84%EF%BC%89mcp%E3%81%A8%E3%81%AF%EF%BC%9F

# è©¦ã—ãŸå†…å®¹

ã“ã¡ã‚‰ã®GitHubã«SLMÃ—MCPé€£æºã®ãƒ„ãƒ¼ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
https://github.com/nomhiro/slm-mcp

ã“ã‚“ãªæ„Ÿã˜ã§å‹•ä½œã—ã¾ã™ã€‚
æŒ‡ç¤ºã«å¾“ã„MCPã®ãƒ„ãƒ¼ãƒ«ã‚’ä»‹ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿ã€æŒ‡ç¤ºã«å¿œç­”ã—ã¾ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®SLMã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ã‚‚å‹•ä½œã—ã¾ã™ã€‚
```powershell
ðŸ’¬ ã‚ãªãŸ: C:\Users\nom40\Documents\PoC\20_slm\slm-mcp\README.mdã€€ã®å†…å®¹ã‚’100å­—ã§è¦ç´„ã—ã¦æ•™ãˆã¦

ðŸ”„ è€ƒãˆã¦ã„ã¾ã™...

ðŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: 2025-07-31 22:11:42,276 - httpx - INFO - HTTP Request: POST http://localhost:5273/v1/chat/completions "HTTP/1.1 200 OK"
ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ãŸã ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿å–ã‚‹ãŸã‚ã®æŒ‡ç¤ºã‚’æä¾›ã§ãã¾ã™:

[{"name": "read_file", "parameters": {"path": "C:\\Users\\nom40\\Documents\\PoC\\20_slm\\slm-mcp\\README.md"}}]


ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’100å­—ã§è¦ç´„ã™ã‚‹ã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã£ãŸå¾Œã€è¦ç´„ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯AIã¨ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ç›´æŽ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚è¦ç´„ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®æ‰‹é †ã¯ã€ä¸Šè¨˜ã®JSONã‚ªãƒ–ã‚¸ ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å–å¾—ã™ã‚‹ã“ã¨ã§ã™ã€‚

ðŸ”§ Phi ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œç¢ºèª
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ãƒ„ãƒ¼ãƒ«: read_file
èª¬æ˜Ž: Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Use the 'head' parameter to read only the first N lines of a file, or the 'tail' parameter to read only the last N lines of a file. Only works within allowed directories.       
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
{
  "path": "C:\\Users\\nom40\\Documents\\PoC\\20_slm\\slm-mcp\\README.md"
}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


é¸æŠžã—ã¦ãã ã•ã„:
  [Y] ã¯ã„ - ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
  [N] ã„ã„ãˆ - å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
  [S] è©³ç´° - å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèª

å…¥åŠ› (Y/N/S): Y
2025-07-31 22:12:04,674 - slm_mcp.core.mcp_manager - INFO - Tool 'read_file' executed successfully in 2.74s
âœ… ãƒ„ãƒ¼ãƒ«å®Ÿè¡ŒæˆåŠŸ: read_file

ðŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: 2025-07-31 22:13:08,818 - httpx - INFO - HTTP Request: POST http://localhost:5273/v1/chat/completions "HTTP/1.1 200 OK"
æä¾›ã•ã‚ŒãŸ `read_file` ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæžœã«åŸºã¥ã„ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ« `README.md` ã®è¦ç´„ã¯æ¬¡ã®é€šã‚Šã§ã™:

ãƒ­ãƒ¼ã‚«ãƒ«å°è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ï¼ˆSLMï¼‰ã¨Model Context Protocolï¼ˆMCPï¼‰ã®çµ±åˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
SLM-MCPã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œã™ã‚‹Phiã‚·ãƒªãƒ¼ã‚ºï¼ˆPhi-4ã€Phi-4-miniç­‰ï¼‰ã®å°è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã¨ã€MCPã‚µãƒ¼ãƒãƒ¼ã‚’é€£æºã•ã›ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ“ä½œãªã©ã®è±Šå¯Œãªæ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã™ã€‚
```

# å…ˆã«çµè«–

phi-4-mini ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç­‰ã®æŒ‡ç¤ºã®åŠ¹ãã«ãã•ã¨ã€gpt-3.5æ™‚ä»£ã‚’æ€ã„å‡ºã™æŽ¨è«–èƒ½åŠ›ã ãª ã¨ã„ã†æ„Ÿè§¦ã§ã™ã€‚
åˆ©ç”¨ã‚·ãƒ¼ãƒ³ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒãªã©ã¨é™å®šçš„ã§ã™ã€‚ãŒã€ã€è©¦ã—ã¦ã¿ã¾ã—ãŸã®ã§ã©ã“ã‹ã§å‚è€ƒã«ãªã‚Œã°ã¨æ€ã„ã¾ã™ã€‚

# ãƒ„ãƒ¼ãƒ«ã®è§£èª¬

### çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆæ€æƒ³

MCPä»¥å‰ã¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒ„ãƒ¼ãƒ«ãŒå¯†æŽ¥ã«çµã³ã¤ã„ã¦ã„ã¾ã—ãŸãŒã€ãƒ„ãƒ¼ãƒ«ã¨AIãƒ¢ãƒ‡ãƒ«ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚

- MCPã«ã‚ˆã‚Šãƒ„ãƒ¼ãƒ«ä¸€è¦§ã¨ãƒ„ãƒ¼ãƒ«èª¬æ˜Žã®æƒ…å ±ã‚’äº¤æ›
- SLMãŒè‡ªç„¶è¨€èªžã§ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æ„å›³ã‚’æŠ½å‡º
- è‡ªç„¶è¨€èªžã®æ„å›³ã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸAPIã‚³ãƒ¼ãƒ«ã«å¤‰æ›
- å®Ÿéš›ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã¨ãƒªã‚½ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹
- ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæžœã‚’è‡ªç„¶è¨€èªžã§è§£é‡ˆã—ã¦æç¤º

# ãƒ„ãƒ¼ãƒ«èµ·å‹•æ–¹æ³•

## ç’°å¢ƒæ§‹ç¯‰

ã¾ãšã€å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚
ã“ã“ã§ã¯ã€ä¾‹ã¨ã—ã¦filesystemMCPã‚’åˆ©ç”¨ã—ã¾ã™ã€‚

```bash
# Pythonç’°å¢ƒã®æº–å‚™
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install openai mcp pydantic httpx

# MCP Filesystem Serverã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install -g @modelcontextprotocol/server-filesystem
```

## SLMã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

phi-4-miniã‚’ä½¿ã„ã¾ã™ã€‚FoundryLocalã‚’ä½¿ç”¨ã—ã¦Phi-4-miniã‚’èµ·å‹•ã—ã¾ã™ã€‚
SLMãŒå•é¡Œãªãå‹•ä½œã—ã¦ã„ã‚‹ã‹ã€ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

â–¼ Phi-4-miniãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å®Ÿè¡Œã¾ã§è¡Œã†å ´åˆ
```bash
foundry model run phi-4-mini
```

â–¼ phi-4-mini ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹ã‚’åˆ†ã‘ã¦å®Ÿè¡Œã™ã‚‹å ´åˆ
```bash
# 1. ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
foundry model download phi-4-mini

# 2. ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
foundry service start

# 3. ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
foundry model load phi-4-mini
```

## MCPã‚µãƒ¼ãƒãƒ¼ã®è¨­å®š

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`config.json`ï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚
`/path/to/allowed/directory` ã¯ãŠä½¿ã„ã®ç’°å¢ƒã«åˆã‚ã›ã¦ã€filesystemMCPã«æ“ä½œã‚’è¨±å¯ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

```json
{
  "slm": {
    "alias": "phi-4-mini",
    "base_url": "http://localhost:8080/v1",
    "api_key": "local"
  },
  "mcp_servers": [
    {
      "name": "filesystem",
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-filesystem",
        "/path/to/allowed/directory"
      ],
      "allowed_paths": [
        "/path/to/allowed/directory"
      ]
    }
  ],
  "settings": {
    "system_message": "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¿œãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
  }
}
```

## èµ·å‹•

`--config`ã§ä¸€ã¤å‰ã®æ‰‹é †ã§å®šç¾©ã—ãŸconfigã®Jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
â€»æŒ‡å®šã—ã¦ã„ãªã„å ´åˆã¯default_config.jsonãŒä½¿ã‚ã‚Œã¾ã™ã€‚
```
python .\run_slm_mcp.py --config test_config.json
```

ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚Œã°å•é¡Œãªãèµ·å‹•ã§ãã¦ã„ã¾ã™ã€‚
```powershell
(.venv) 20_slm\slm-mcp > python .\run_slm_mcp.py --config test_config.json
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
2025-07-31 22:31:16,652 - slm_mcp.cli.main - INFO - Initializing SLM client with alias: phi-4-mini
2025-07-31 22:31:18,882 - httpx - INFO - HTTP Request: GET http://localhost:5273/foundry/list "HTTP/1.1 200 OK"
2025-07-31 22:31:19,166 - httpx - INFO - HTTP Request: GET http://localhost:5273/openai/models "HTTP/1.1 200 OK"
2025-07-31 22:31:23,315 - httpx - INFO - HTTP Request: GET http://localhost:5273/openai/load/Phi-4-mini-instruct-generic-cpu?ttl=600 "HTTP/1.1 200 OK"
2025-07-31 22:31:23,606 - slm_mcp.cli.main - INFO - Connecting to 1 MCP servers...
2025-07-31 22:31:26,113 - slm_mcp.core.mcp_manager - INFO - Retrieved 12 tools from filesystem
2025-07-31 22:31:26,153 - slm_mcp.core.mcp_manager - INFO - Connected to MCP server 'filesystem' with 12 tools
2025-07-31 22:31:26,153 - slm_mcp.core.mcp_manager - INFO - Connected to 1/1 MCP servers
2025-07-31 22:31:26,155 - slm_mcp.core.mcp_manager - INFO - Available tools: ['read_file', 'read_multiple_files', 'write_file', 'edit_file', 'create_directory', 'list_directory', 'list_directory_with_sizes', 'directory_tree', 'move_file', 'search_files', 'get_file_info', 'list_allowed_directories']

ðŸ¤– SLM MCPçµ±åˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ“¡ SLMãƒ¢ãƒ‡ãƒ«: phi-4-mini
ðŸ”— MCPã‚µãƒ¼ãƒãƒ¼: 1 æŽ¥ç¶š
ðŸ› ï¸  åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«: 12 å€‹


ðŸ“– èª­ã¿å–ã‚Šç³»:
  â€¢ read_file: Read the complete contents of a file from the file...
  â€¢ read_multiple_files: Read the contents of multiple files simultaneously...

âœï¸ æ›¸ãè¾¼ã¿ç³»:
  â€¢ write_file: Create a new file or completely overwrite an exist...
  â€¢ edit_file: Make line-based edits to a text file. Each edit re...

ðŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç³»:
  â€¢ create_directory: Create a new directory or ensure a directory exist...
  â€¢ list_directory: Get a detailed listing of all files and directorie...
  â€¢ list_directory_with_sizes: Get a detailed listing of all files and directorie...
  â€¢ ... ä»– 2 å€‹

ðŸ”§ ãã®ä»–:
  â€¢ move_file: Move or rename files and directories. Can move fil...
  â€¢ search_files: Recursively search for files and directories match...
  â€¢ get_file_info: Retrieve detailed metadata about a file or directo...

ã‚³ãƒžãƒ³ãƒ‰:
  â€¢ exit       - çµ‚äº†
  â€¢ noconfirm  - ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œç¢ºèªã‚’ç„¡åŠ¹åŒ–
  â€¢ confirm    - ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œç¢ºèªã‚’æœ‰åŠ¹åŒ–
  â€¢ tools      - åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’å†è¡¨ç¤º
  â€¢ stats      - å®Ÿè¡Œçµ±è¨ˆã‚’è¡¨ç¤º
  â€¢ clear      - ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
  â€¢ config     - è¨­å®šã‚’è¡¨ç¤º
  â€¢ addpath    - è¨±å¯ã•ã‚ŒãŸãƒ‘ã‚¹ã‚’è¿½åŠ 
  â€¢ listpaths  - è¨±å¯ã•ã‚ŒãŸãƒ‘ã‚¹ã‚’è¡¨ç¤º
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ðŸ’¬ ã‚ãªãŸ:
```

# å®Ÿè£…è§£èª¬

## ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®å®Ÿè£…

SLMã‹ã‚‰MCPãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ãŸã‚ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

```python
class PhiToolOrchestrator:
    def __init__(self, mcp_manager, openai_client, model_id):
        self.mcp_manager = mcp_manager
        self.openai_client = openai_client
        self.model_id = model_id
        self.conversation_manager = ConversationManager()
        self.tool_parser = PhiToolParser()
    
    async def execute_conversation_turn(self, user_input: str) -> str:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        self.conversation_manager.add_message("user", user_input)
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
        available_tools = self.mcp_manager.get_available_tools()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’è¿½åŠ 
        tool_prompt = self._create_tool_prompt(available_tools)
        
        # LLMã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        response = await self._stream_llm_response()
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è§£æž
        tool_calls = self.tool_parser.parse_tool_calls(response, self.mcp_manager.tools)
        
        # ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
        if tool_calls:
            tool_results = await self._execute_tools(tool_calls)
            # çµæžœã‚’åŸºã«æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
            final_response = await self._generate_final_response(tool_results)
            return final_response
        
        return response
```

### ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ‘ãƒ¼ã‚µãƒ¼ã®å®Ÿè£…è©³ç´°

SLMã®å‡ºåŠ›ã‹ã‚‰ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®å†…å®¹ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™ã€‚

```python
class PhiToolParser:
    def __init__(self):
        self.tool_pattern = re.compile(
            r'\[{"name":\s*"([^"]+)",\s*"parameters":\s*({[^}]+})\}]',
            re.DOTALL
        )
    
    def parse_tool_calls(self, response: str, available_tools: Dict[str, MCPTool]) -> List[ToolCall]:
        """SLMã®å¿œç­”ã‹ã‚‰ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’æŠ½å‡º"""
        tool_calls = []
        
        # è¤‡æ•°ã®å‘¼ã³å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
        matches = self.tool_pattern.finditer(response)
        
        for match in matches:
            tool_name = match.group(1)
            params_str = match.group(2)
            
            # JSONãƒ‘ãƒ¼ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            try:
                parameters = json.loads(params_str)
            except json.JSONDecodeError:
                # ä¿®å¾©å¯èƒ½ãªJSONã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ä¿®æ­£ã‚’è©¦ã¿ã‚‹
                parameters = self._repair_json(params_str)
            
            # ãƒ„ãƒ¼ãƒ«ã®å¦¥å½“æ€§æ¤œè¨¼
            if tool_name in available_tools:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒžã®æ¤œè¨¼
                validated_params = self._validate_parameters(
                    parameters, 
                    available_tools[tool_name].input_schema
                )
                
                tool_calls.append(ToolCall(
                    name=tool_name,
                    parameters=validated_params
                ))
        
        return tool_calls
    
    def _validate_parameters(self, params: dict, schema: dict) -> dict:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒ¼ãƒžã«å¯¾ã—ã¦æ¤œè¨¼ãƒ»æ­£è¦åŒ–"""
        # JSON Schemaã‚’ä½¿ç”¨ã—ãŸæ¤œè¨¼
        from jsonschema import validate, ValidationError
        
        try:
            validate(instance=params, schema=schema)
            return params
        except ValidationError as e:
            # å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•è£œå®Œãªã©
            return self._apply_default_values(params, schema)
```

## ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ

SLMã‹ã‚‰ã®å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

```python
async def _stream_llm_response(self) -> str:
    assistant_content = ""
    print("\nðŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: ", end="", flush=True)
    
    response = self.openai_client.chat.completions.create(
        model=self.model_id,
        messages=self.conversation_manager.conversation_history,
        stream=True
    )
    
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            content_chunk = chunk.choices[0].delta.content
            assistant_content += content_chunk
            print(content_chunk, end="", flush=True)
    
    print()
    return assistant_content
```

